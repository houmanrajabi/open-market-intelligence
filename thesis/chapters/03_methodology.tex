\chapter{Methodology and System Architecture}
\label{ch:methodology}

\section{System Overview}

This research implements a "Self-Monitoring Compliance Assistant," a Retrieval-Augmented Generation (RAG) system designed to balance answer quality with strict reliability signals. The architecture is composed of four distinct layers:
\begin{enumerate}
    \item \textbf{Data Processing Layer:} A hybrid visual-textual extraction pipeline that parses unstructured PDFs into semantically rich chunks.
    \item \textbf{Retrieval Layer:} A dense vector index allowing for semantic search over FOMC documents.
    \item \textbf{Uncertainty-Aware Generation Layer:} A Large Language Model (LLM) equipped with token-level entropy monitoring to detect potential hallucinations.
    \item \textbf{Alignment Layer:} An offline Reinforcement Learning from AI Feedback (RLAIF) loop utilizing a "Teacher" model to optimize citation accuracy.
\end{enumerate}



\section{Data Processing Pipeline}

High-quality retrieval is contingent upon high-quality data ingestion. Standard text extraction libraries often fail to preserve the structural semantics of complex regulatory documents, such as tables and multi-column layouts. To address this, we developed a \textbf{Hybrid Visual-Textual Extractor}.

\subsection{Hybrid Extraction Strategy}
The extraction pipeline, implemented in Python, utilizes a two-pass approach for every document page:

\subsubsection{Pass 1: Visual Layout Analysis}
The system first converts PDF pages into high-resolution images. These images are processed by a Vision Language Model (VLM), specifically \texttt{Qwen2-VL-72B-Instruct}, which acts as a "Document Structure Layout Engine". The VLM is prompted to identify bounding boxes for five specific element types:
\begin{itemize}
    \item \textbf{HEADER:} Section titles and table captions.
    \item \textbf{TABLE:} Rows and columns containing numerical data.
    \item \textbf{FIGURE:} Charts, graphs, and visual diagrams.
    \item \textbf{FOOTER:} Page numbers and non-content markers.
    \item \textbf{TEXT:} Standard paragraphs and bullet points.
\end{itemize}

Crucially, the system employs an exponential backoff retry mechanism (up to 3 attempts) to handle potential VLM inference failures, ensuring pipeline robustness.

\subsubsection{Pass 2: Content Extraction}
Once the layout is mapped, the system iterates through the detected elements using an "Intelligent Reading Order" algorithm that sorts elements by column and vertical position.
\begin{itemize}
    \item \textbf{Text Elements:} Extracted directly using \texttt{PyMuPDF} to ensure perfect character accuracy.
    \item \textbf{Visual Elements (Tables/Charts):} The system crops the image to the detected bounding box and re-submits it to the VLM. Tables are transcribed into strictly valid CSV format, while charts are summarized into structured JSON containing titles, axes labels, and key insights.
\end{itemize}

\section{Indexing and Chunking}

\subsection{Intelligent Chunking}
To optimize for retrieval relevance, we implement an \texttt{IntelligentChunker} rather than a fixed-window sliding splitter. This module aggregates sequential elements into chunks based on token count (Target: 512 tokens, Max: 1024 tokens) while respecting semantic boundaries.

If a single element (e.g., a long paragraph) exceeds the maximum chunk size, it is split by sentence boundaries; otherwise, elements are merged until the target size is reached.

\subsection{Vector Embeddings}
Chunks are embedded using OpenAI's \texttt{text-embedding-3-large} model. The resulting vectors are stored in a vector database, enabling cosine similarity search. Top-$k$ retrieval is set initially to $k=5$, with logic to expand to $k=10$ if the uncertainty module triggers a low-confidence flag.

\section{Uncertainty Module: Entropy-Based Monitoring}

The core novelty of this system is the \textbf{Uncertainty Module}. Unlike standard RAG systems that output a deterministic answer, our system calculates the Shannon entropy $H(t)$ for the generated token sequence:
\begin{equation}
    H(t) = -\sum_{w \in V} p(w) \log p(w)
\end{equation}
Where $V$ is the vocabulary and $p(w)$ is the probability distribution over the next token.

The system calculates the mean entropy across the generated answer (excluding citations). A threshold $H_{threshold}$ (provisionally set to 1.5) serves as a gate:
\begin{itemize}
    \item \textbf{Low Entropy ($< 1.5$):} The answer is returned to the user.
    \item \textbf{High Entropy ($\ge 1.5$):} The system triggers a "re-prompt" with expanded retrieval ($k=10$) or returns an abstention message ("Insufficient information").
\end{itemize}

\section{Alignment: Offline DPO}

To improve the model's adherence to citation constraints, we employ Direct Preference Optimization (DPO). A "Teacher" model (GPT-4-Turbo) grades generated answers on a 0-10 scale across four dimensions: Factual Grounding, Citation Accuracy, Completeness, and Clarity.

Pairs of (Winner, Loser) answers are collected to fine-tune the student model (Llama-3-8B), specifically optimizing for the reduction of hallucinated content and the precise formatting of citations.