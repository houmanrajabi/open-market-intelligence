\chapter{System Design and Implementation}
\label{ch:system_design}

This chapter details the engineering specifications of the "Self-Monitoring Compliance Assistant." It describes the architectural decisions, data pipelines, and software components developed to implement the hybrid visual-textual extraction and entropy-aware generation mechanisms.

\section{Architectural Overview}

The system is architected as a modular pipeline consisting of three decoupled subsystems: the \textbf{Ingestion Subsystem}, the \textbf{Inference Subsystem}, and the \textbf{Alignment Subsystem}. This decoupling allows for independent scaling of the computationally expensive vision processing components from the latency-sensitive retrieval components.

% \begin{figure}[h]
%     \centering
%     % \includegraphics[width=\textwidth]{figures/system_architecture_diagram.png}
%     \caption{High-level system architecture showing the flow from Raw PDF to RLAIF-optimized Generation.}
%     \label{fig:system_arch}
% \end{figure}

\begin{figure}[ht]
    \centering
    \resizebox{\textwidth}{!}{
        \input{figures/system_diagram.tex}
    }
    \caption{Architecture of the Self-Monitoring Compliance Assistant. The diagram illustrates the Hybrid Visual Ingestion pipeline (top), the Entropy-Aware Inference loop (bottom), and the offline DPO Alignment signal (dashed).}
    \label{fig:system_arch}
\end{figure}

\section{Subsystem 1: The Visual-First Ingestion Pipeline}
Unlike traditional RAG systems that rely on text-only parsing (e.g., \texttt{pypdf}), this system implements a "Visual-First" strategy to preserve the semantic structure of regulatory documents.

\subsection{Hybrid Extraction Engine}
The extraction logic is encapsulated in the \texttt{HybridExtractor} module. It orchestrates a two-pass process for every PDF page:

\subsubsection{Pass 1: Visual Layout Analysis (VLM)}
The page is rendered as a high-resolution image and passed to \texttt{Qwen2-VL-72B-Instruct}. We utilize a custom prompt optimized for financial document structure, defining five detection classes: \texttt{HEADER}, \texttt{TABLE}, \texttt{FIGURE}, \texttt{FOOTER}, and \texttt{TEXT}.
\begin{itemize}
    \item \textbf{Robustness:} An exponential backoff retry mechanism handles potential API timeouts during heavy inference loads.
    \item \textbf{Coordinate Mapping:} Bounding boxes returned by the VLM in normalized $[0, 1000]$ space are scaled to the absolute PDF coordinates for precise cropping.
\end{itemize}

\subsubsection{Pass 2: Content Parsing}
Once the layout is established, the system applies specific handlers based on the element type:
\begin{itemize}
    \item \textbf{Text Blocks:} Extracted via \texttt{PyMuPDF} for perfect OCR-free character fidelity.
    \item \textbf{Tables:} The bounding box is cropped and re-sent to the VLM with a strict "CSV-Only" prompt, ensuring tables are digitized as structured data rather than unstructured text blobs.
    \item \textbf{Charts:} Visual charts are summarized into JSON objects containing titles, axes labels, and key trends, making visual data accessible to the text-based embedding model.
\end{itemize}

\subsection{Intelligent Chunking Strategy}
Data is segmented using the \texttt{IntelligentChunker} class. Instead of a sliding window, this component aggregates sequential elements (e.g., a header followed by three paragraphs) until a token threshold (Target: 512) is met. This ensures that chunks represent coherent semantic units rather than arbitrary text slices.

\section{Subsystem 2: Uncertainty-Aware Inference Engine}

The runtime engine is built on the Llama-3-8B-Instruct architecture, modified to expose token-level log-probabilities during generation.

\subsection{Entropy Calculation Kernel}
For every generated token $t_i$ in the output sequence, the system captures the probability distribution $P(t_i | t_{<i}, C)$ over the vocabulary $V$. The Shannon entropy $H(t_i)$ is computed in real-time:
\begin{equation}
    H(t_i) = -\sum_{v \in V} P(v) \log P(v)
\end{equation}
The system computes the arithmetic mean of $H(t)$ across the generated response. A configurable threshold $\tau$ (default $\tau=1.5$) determines the "Uncertainty Gate" logic.

\subsection{The Retrieval-Generation Loop}
The inference flow follows a conditional logic path:
\begin{enumerate}
    \item \textbf{Initial Retrieval:} Query vector search retrieves top-$k=5$ chunks.
    \item \textbf{Generation:} Llama-3 generates a candidate response.
    \item \textbf{Entropy Check:}
    \begin{itemize}
        \item If $\bar{H} < \tau$: Return response to user.
        \item If $\bar{H} \ge \tau$: Trigger \textbf{Expanded Retrieval} ($k=10$) and re-generate with a "Review this carefully" system prompt.
    \end{itemize}
    \item \textbf{Abstention:} If $\bar{H} \ge \tau$ persists after re-generation, return the "Insufficient Information" token.
\end{enumerate}

\section{Subsystem 3: Offline Alignment (RLAIF)}

To enable continuous improvement without expensive human annotation, the system implements an offline Direct Preference Optimization (DPO) loop.

\subsection{Teacher-Student Grading Protocol}
A "Teacher" agent (GPT-4-Turbo) evaluates the Student's (Llama-3) outputs against the retrieved context. The grading schema enforces:
\begin{itemize}
    \item \textbf{Citation Precision:} Does the cited chunk ID explicitly contain the claim?
    \item \textbf{Negative Constraints:} Does the answer avoid information not present in the chunks?
\end{itemize}
Outputs scoring $\ge 8/10$ are labeled as "Chosen," while those scoring $\le 5/10$ or containing hallucinations are labeled as "Rejected".

\subsection{Data Storage Schema}
All interaction data is serialized in a structured JSON format to support the training pipeline:
\begin{verbatim}
{
  "doc_id": "FOMC20230614",
  "chunks": [
    {
      "chunk_id": "chunk_12",
      "content": "...",
      "metadata": {
        "page": 4,
        "type": "TABLE",
        "entropy_score": 0.42
      }
    }
  ]
}
\end{verbatim}

\section{Implementation Details}

The system is implemented in Python 3.10. The VLM inference is managed via an OpenAI-compatible API wrapper serving the quantized \texttt{Qwen2-VL} model. Vector storage is handled by ChromaDB for persistent, local embedding management. The entire pipeline is containerized to ensure reproducibility across development and training environments.