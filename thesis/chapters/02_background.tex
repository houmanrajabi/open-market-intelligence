\chapter{Background and Related Work}
\label{ch:background}

This chapter establishes the theoretical foundations for the "Self-Monitoring Compliance Assistant." It begins by characterizing the unique challenges of regulatory compliance. It then briefly reviews the foundational neural architectures that underpin modern AI—specifically Convolutional Neural Networks and Transformers—before detailing the specific frameworks used in this thesis: Retrieval-Augmented Generation (RAG), uncertainty estimation, and Reinforcement Learning from AI Feedback (RLAIF).

\section{The Challenge of Regulatory Compliance}
% [Content from previous draft remains here: Information Overload, Necessity of Grounding]
Regulatory compliance is a high-stakes domain characterized by dense, technical language and a constantly evolving corpus of documents. Unlike general-domain Question Answering (QA), compliance tasks require near-zero tolerance for fabrication ("hallucination") and a strict adherence to source texts.

\section{Foundational Neural Architectures}

To understand the multimodal and generative capabilities of the proposed system, it is necessary to briefly review the underlying deep learning architectures employed in both the extraction and generation phases.

\subsection{Convolutional Neural Networks (CNNs)}
Convolutional Neural Networks (CNNs) represent a class of deep neural networks designed to process grid-like data topologies, such as images. By employing convolutional layers that apply learnable filters to input data, CNNs effectively capture spatial hierarchies and local dependencies. In this research, CNN-based principles are relevant to the Visual Language Model (VLM) pipeline used for document layout analysis, specifically in the feature extraction phase where visual elements like tables and charts are identified before textual processing.

\subsection{The Transformer and Self-Attention}
The Transformer architecture revolutionized Natural Language Processing by discarding recurrence in favor of the \textit{Self-Attention} mechanism. Attention allows the model to weigh the significance of different words in a sequence relative to one another, regardless of their distance.
\begin{equation}
    Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
This mechanism enables the modeling of long-range dependencies crucial for understanding complex legal texts. The Llama-3 model used in this thesis is a decoder-only Transformer, leveraging this architecture to generate coherent, context-aware responses based on the retrieved compliance documents. \cite{lecun1989backpropagation}

\section{Retrieval-Augmented Generation (RAG)}

Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for grounding LLMs in external knowledge.

\subsection{Standard Architecture}
A standard RAG system consists of a dense vector index (typically using embeddings like OpenAI's \texttt{text-embedding-3}) and a generative model. The process follows a "Retrieve-then-Generate" workflow:
\begin{enumerate}
    \item \textbf{Query Encoding:} The user's question is converted into a vector.
    \item \textbf{Retrieval:} The system fetches the top-$k$ most similar document chunks (e.g., 512-token segments) via cosine similarity.
    \item \textbf{Generation:} The retrieved chunks are prepended to the system prompt, and the model generates an answer conditioned on this context.
\end{enumerate}



\subsection{Limitations of Naive RAG}
While effective, naive RAG systems suffer from specific failure modes in high-stakes domains:
\begin{itemize}
    \item \textbf{Retrieval Failure:} If the relevant information is not in the top-$k$ chunks, the model may hallucinate an answer based on its pre-training weights rather than admitting ignorance.
    \item \textbf{Contextual Misinterpretation:} Models may struggle with "negation traps" or "temporal confusion," citing a correct document but misinterpreting its legal implication.
\end{itemize}

\section{Uncertainty Estimation in LLMs}

Reliable deployment of LLMs in compliance requires the system to "know when it doesn't know."

\subsection{Types of Uncertainty}
Uncertainty in machine learning is typically categorized into two types:
\begin{itemize}
    \item \textbf{Aleatoric Uncertainty:} Inherent noise in the data itself.
    \item \textbf{Epistemic Uncertainty:} The model's lack of knowledge about a specific topic.
\end{itemize}
In the context of RAG, we are primarily concerned with epistemic uncertainty—detecting when the retrieved context is insufficient to answer the query.

\subsection{Entropy-Based Monitoring}
A practical proxy for uncertainty is \textbf{Shannon Entropy}. For a generated sequence of tokens, high entropy ($H(t)$) often correlates with low confidence or confusion.
\begin{equation}
    H(t) = -\sum_{w \in V} p(w) \log p(w)
\end{equation}
By monitoring the mean entropy of an answer, a system can establish a dynamic threshold (e.g., $H_{threshold} = 1.5$) to flag potentially unreliable responses. This thesis investigates whether this token-level metric can serve as a reliable gate for "abstention".

\section{Automated Alignment: From RLHF to RLAIF}

To mitigate hallucinations, LLMs are traditionally fine-tuned using Reinforcement Learning from Human Feedback (RLHF). However, RLHF is resource-intensive and difficult to scale.

\subsection{Reinforcement Learning from AI Feedback (RLAIF)}
Recent research proposes replacing the human annotator with a stronger "Teacher" model (e.g., GPT-4). This approach, known as RLAIF, allows for rapid, low-cost alignment.

\subsection{Direct Preference Optimization (DPO)}
A streamlined alternative to PPO (Proximal Policy Optimization), DPO (Direct Preference Optimization) optimizes the model directly on preference pairs (Winner vs. Loser) without needing a separate reward model. In this thesis, we utilize an offline DPO protocol where a Teacher model grades Student answers on strict criteria: factual grounding, citation accuracy, and refusal appropriateness.