"""
DPO (Direct Preference Optimization) Trainer

This module implements offline DPO training for Llama-3-8B-Instruct using
preference pairs generated by the Teacher model.

DPO Loss:
    L = -log(σ(β * (r_θ(x, y_w) - r_θ(x, y_l))))

Where:
    - r_θ(x, y) = log(π_θ(y|x) / π_ref(y|x))  # Reward from policy vs reference
    - y_w = chosen (win) response
    - y_l = rejected (loss) response
    - β = KL penalty coefficient
    - σ = sigmoid function

Requirements:
    pip install transformers trl peft accelerate bitsandbytes
"""

import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging

import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig
)
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

from src.utils.config import config
from src.utils.logger import logger


@dataclass
class DPOTrainingConfig:
    """Configuration for DPO training (values default to config.alignment)"""
    # Model
    model_name: Optional[str] = None
    reference_model: Optional[str] = None  # Use same as model if None

    # DPO hyperparameters
    beta: Optional[float] = None
    learning_rate: Optional[float] = None
    num_train_epochs: Optional[int] = None
    per_device_train_batch_size: Optional[int] = None
    gradient_accumulation_steps: Optional[int] = None

    # LoRA configuration (for efficient fine-tuning)
    use_lora: Optional[bool] = None
    lora_r: Optional[int] = None
    lora_alpha: Optional[int] = None
    lora_dropout: Optional[float] = None
    lora_target_modules: Optional[List[str]] = None

    # Quantization (for memory efficiency)
    use_4bit: Optional[bool] = None
    bnb_4bit_compute_dtype: Optional[str] = None

    # Output
    output_dir: Optional[Path] = None
    save_steps: Optional[int] = None
    logging_steps: Optional[int] = None

    # Training
    max_length: Optional[int] = None
    max_prompt_length: Optional[int] = None
    warmup_steps: Optional[int] = None
    fp16: Optional[bool] = None
    bf16: Optional[bool] = None

    def __post_init__(self):
        # Use config defaults for None values
        if self.model_name is None:
            self.model_name = config.alignment.student_model_name
        if self.beta is None:
            self.beta = config.alignment.dpo_beta
        if self.learning_rate is None:
            self.learning_rate = config.alignment.dpo_learning_rate
        if self.num_train_epochs is None:
            self.num_train_epochs = config.alignment.dpo_num_epochs
        if self.per_device_train_batch_size is None:
            self.per_device_train_batch_size = config.alignment.dpo_batch_size
        if self.gradient_accumulation_steps is None:
            self.gradient_accumulation_steps = config.alignment.dpo_gradient_accumulation_steps
        if self.use_lora is None:
            self.use_lora = config.alignment.use_lora
        if self.lora_r is None:
            self.lora_r = config.alignment.lora_r
        if self.lora_alpha is None:
            self.lora_alpha = config.alignment.lora_alpha
        if self.lora_dropout is None:
            self.lora_dropout = config.alignment.lora_dropout
        if self.lora_target_modules is None:
            self.lora_target_modules = config.alignment.lora_target_modules
        if self.use_4bit is None:
            self.use_4bit = config.alignment.use_4bit
        if self.bnb_4bit_compute_dtype is None:
            self.bnb_4bit_compute_dtype = config.alignment.bnb_4bit_compute_dtype
        if self.output_dir is None:
            self.output_dir = config.alignment.aligned_model_output_dir
        if self.save_steps is None:
            self.save_steps = config.alignment.dpo_save_steps
        if self.logging_steps is None:
            self.logging_steps = config.alignment.dpo_logging_steps
        if self.max_length is None:
            self.max_length = config.alignment.dpo_max_length
        if self.max_prompt_length is None:
            self.max_prompt_length = config.alignment.dpo_max_prompt_length
        if self.warmup_steps is None:
            self.warmup_steps = config.alignment.dpo_warmup_steps
        if self.fp16 is None:
            self.fp16 = config.alignment.dpo_fp16
        if self.bf16 is None:
            self.bf16 = config.alignment.dpo_bf16

        self.output_dir = Path(self.output_dir)


class DPOTrainerWrapper:
    """
    Wrapper for DPO training with Llama-3.

    This handles:
    1. Loading preference pairs
    2. Preparing model with quantization/LoRA
    3. Running DPO training
    4. Saving aligned model
    """

    def __init__(self, config: DPOTrainingConfig = None):
        """
        Initialize DPO trainer.

        Args:
            config: Training configuration
        """
        self.config = config or DPOTrainingConfig()
        self.config.output_dir.mkdir(parents=True, exist_ok=True)

        self.model = None
        self.ref_model = None
        self.tokenizer = None

        logger.info(f"DPO Trainer initialized: {self.config.model_name}")

    def load_preference_pairs(
        self,
        pairs_file: Path
    ) -> Dataset:
        """
        Load preference pairs and convert to HuggingFace Dataset.

        Expected format:
        [
            {"prompt": "...", "chosen": "...", "rejected": "..."},
            ...
        ]

        Args:
            pairs_file: JSON file with preference pairs

        Returns:
            HuggingFace Dataset
        """
        logger.info(f"Loading preference pairs from: {pairs_file}")

        with open(pairs_file, 'r', encoding='utf-8') as f:
            pairs = json.load(f)

        # Convert to Dataset
        dataset = Dataset.from_dict({
            "prompt": [p["prompt"] for p in pairs],
            "chosen": [p["chosen"] for p in pairs],
            "rejected": [p["rejected"] for p in pairs]
        })

        logger.info(f"Loaded {len(dataset)} preference pairs")

        return dataset

    def prepare_model(self):
        """
        Prepare model for training with quantization and LoRA.
        """
        logger.info(f"Loading model: {self.config.model_name}")

        # Quantization config
        if self.config.use_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=getattr(torch, self.config.bnb_4bit_compute_dtype),
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            logger.info("Using 4-bit quantization")
        else:
            bnb_config = None

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"

        # Load model
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config.bf16 else torch.float16
        )

        # Prepare for k-bit training
        if self.config.use_4bit:
            self.model = prepare_model_for_kbit_training(self.model)

        # Apply LoRA
        if self.config.use_lora:
            lora_config = LoraConfig(
                r=self.config.lora_r,
                lora_alpha=self.config.lora_alpha,
                target_modules=self.config.lora_target_modules,
                lora_dropout=self.config.lora_dropout,
                bias="none",
                task_type="CAUSAL_LM"
            )
            self.model = get_peft_model(self.model, lora_config)
            logger.info(f"Applied LoRA: r={self.config.lora_r}, alpha={self.config.lora_alpha}")

        # Load reference model (for DPO)
        # Reference model uses same architecture but frozen weights
        ref_model_name = self.config.reference_model or self.config.model_name
        self.ref_model = AutoModelForCausalLM.from_pretrained(
            ref_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config.bf16 else torch.float16
        )

        logger.info("Models loaded successfully")

    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None
    ):
        """
        Run DPO training.

        Args:
            train_dataset: Training preference pairs
            eval_dataset: Optional evaluation pairs
        """
        if self.model is None:
            raise ValueError("Model not prepared. Call prepare_model() first.")

        logger.info("Starting DPO training...")

        # DPO training arguments
        training_args = DPOConfig(
            output_dir=str(self.config.output_dir),
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            bf16=self.config.bf16,
            fp16=self.config.fp16,
            max_length=self.config.max_length,
            max_prompt_length=self.config.max_prompt_length,
            beta=self.config.beta,
            warmup_steps=self.config.warmup_steps,
            optim="paged_adamw_32bit",
            remove_unused_columns=False,
            gradient_checkpointing=True,
            report_to="none"  # Disable wandb/tensorboard for now
        )

        # Initialize DPO trainer
        trainer = DPOTrainer(
            model=self.model,
            ref_model=self.ref_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            beta=self.config.beta
        )

        # Train
        logger.info(f"Training on {len(train_dataset)} pairs for {self.config.num_train_epochs} epochs...")
        trainer.train()

        logger.info("✅ DPO training complete!")

        # Save final model
        self.save_model()

    def save_model(self, output_path: Path = None):
        """
        Save trained model and tokenizer.

        Args:
            output_path: Where to save (uses config.output_dir if None)
        """
        output_path = output_path or self.config.output_dir

        logger.info(f"Saving model to: {output_path}")

        # Save model
        self.model.save_pretrained(output_path)
        self.tokenizer.save_pretrained(output_path)

        logger.info("✅ Model saved successfully")

        # Save training config
        config_path = output_path / "dpo_training_config.json"
        with open(config_path, 'w') as f:
            json.dump({
                "model_name": self.config.model_name,
                "beta": self.config.beta,
                "learning_rate": self.config.learning_rate,
                "num_epochs": self.config.num_train_epochs,
                "lora_r": self.config.lora_r,
                "lora_alpha": self.config.lora_alpha
            }, f, indent=2)


# Example usage
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("=" * 70)
    print("DPO Trainer - Example Usage")
    print("=" * 70)

    print("\nDPO Training Pipeline:")
    print("1. Generate preference pairs (preference_generator.py)")
    print("2. Load pairs and prepare model")
    print("3. Train with DPO objective")
    print("4. Save aligned model")

    print("\nRequirements:")
    print("- GPU with 24GB+ VRAM (for 8B model with 4-bit quantization)")
    print("- CUDA 11.8+ with PyTorch 2.0+")
    print("- ~500+ preference pairs for good alignment")

    print("\nExample command:")
    print("""
from src.alignment.dpo_trainer import DPOTrainerWrapper, DPOTrainingConfig

# Configure training
config = DPOTrainingConfig(
    beta=0.1,  # KL penalty (higher = more conservative)
    learning_rate=5e-7,
    num_train_epochs=3,
    output_dir="models/llama3_dpo_aligned"
)

# Initialize trainer
trainer = DPOTrainerWrapper(config)

# Load preference pairs
train_data = trainer.load_preference_pairs("data/alignment/preference_pairs.json")

# Prepare model (with LoRA + 4-bit quantization)
trainer.prepare_model()

# Train
trainer.train(train_data)

# Model saved to models/llama3_dpo_aligned/
""")

    print("\n" + "=" * 70)
    print("NOTE: Actual training requires GPU and takes several hours")
    print("=" * 70)
