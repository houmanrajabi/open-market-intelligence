{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZobqBS3hI46x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Installing dependencies... (This takes about 2-3 minutes)\n",
            "Collecting vllm==0.6.4.post1\n",
            "  Downloading vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (0.2.1)\n",
            "Collecting numpy<2.0.0 (from vllm==0.6.4.post1)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (4.67.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (4.57.3)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (5.29.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (3.13.2)\n",
            "Requirement already satisfied: openai>=1.45.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (2.12.0)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (0.38.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (2.12.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (11.3.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (0.23.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.4.post1)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (0.12.0)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm==0.6.4.post1)\n",
            "  Downloading lm_format_enforcer-0.10.12-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting outlines<0.1,>=0.0.43 (from vllm==0.6.4.post1)\n",
            "  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (3.20.0)\n",
            "Collecting partial-json-parser (from vllm==0.6.4.post1)\n",
            "  Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (26.2.1)\n",
            "Collecting msgspec (from vllm==0.6.4.post1)\n",
            "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting gguf==0.10.0 (from vllm==0.6.4.post1)\n",
            "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (8.7.0)\n",
            "Collecting mistral-common>=1.5.0 (from mistral-common[opencv]>=1.5.0->vllm==0.6.4.post1)\n",
            "  Downloading mistral_common-1.8.8-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (6.0.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (0.8.1)\n",
            "Collecting compressed-tensors==0.8.0 (from vllm==0.6.4.post1)\n",
            "  Downloading compressed_tensors-0.8.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting ray>=2.9 (from vllm==0.6.4.post1)\n",
            "  Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (13.590.44)\n",
            "Collecting torch==2.5.1 (from vllm==0.6.4.post1)\n",
            "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.20.1 (from vllm==0.6.4.post1)\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.28.post3 (from vllm==0.6.4.post1)\n",
            "  Downloading xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (1.17.0)\n",
            "Requirement already satisfied: setuptools>=74.1.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (75.2.0)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.4.post1) (0.123.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->vllm==0.6.4.post1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->vllm==0.6.4.post1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->vllm==0.6.4.post1) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1->vllm==0.6.4.post1)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm==0.6.4.post1) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.4.post1) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.4.post1) (0.0.4)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer<0.11,>=0.10.9->vllm==0.6.4.post1)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm==0.6.4.post1) (25.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm==0.6.4.post1) (4.25.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm==0.6.4.post1)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common[opencv]>=1.5.0->vllm==0.6.4.post1) (4.12.0.88)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.45.0->vllm==0.6.4.post1) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.45.0->vllm==0.6.4.post1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.45.0->vllm==0.6.4.post1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.45.0->vllm==0.6.4.post1) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.45.0->vllm==0.6.4.post1) (1.3.1)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (1.3.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (3.1.2)\n",
            "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (0.60.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (0.37.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (4.0.0)\n",
            "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm==0.6.4.post1)\n",
            "  Downloading pyairports-0.0.1.tar.gz (3.1 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.4.post1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.4.post1) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.4.post1) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.9->vllm==0.6.4.post1) (8.3.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.9->vllm==0.6.4.post1) (1.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.4.post1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.4.post1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.4.post1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.4.post1) (2025.11.12)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.6.0->vllm==0.6.4.post1) (2025.11.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.19.1->vllm==0.6.4.post1) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.2->vllm==0.6.4.post1) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.4.post1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.4.post1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.4.post1) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.4.post1) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.4.post1) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.4.post1) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.4.post1) (1.22.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->vllm==0.6.4.post1) (3.23.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.4.post1) (0.16.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm==0.6.4.post1)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.4.post1) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]->vllm==0.6.4.post1)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm==0.6.4.post1)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.4.post1) (15.0.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.45.0->vllm==0.6.4.post1) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.1->vllm==0.6.4.post1) (1.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm==0.6.4.post1) (2025.9.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm==0.6.4.post1) (0.30.0)\n",
            "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python-headless>=4.0.0 (from mistral-common[opencv]>=1.5.0->vllm==0.6.4.post1)\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (0.70.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.5.1->vllm==0.6.4.post1) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.4.post1) (2025.3)\n",
            "Downloading vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl (198.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.8.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.0/87.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m146.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m822.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.12-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.8-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m146.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.0.46-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (72.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl (10 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m152.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m142.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyairports\n",
            "  Building wheel for pyairports (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyairports: filename=pyairports-0.0.1-py3-none-any.whl size=3524 sha256=de1922e0a995572b2a3d45907c18d27dd568ba2874d74c1f00ff3433f9d63f0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/38/6d/46c9744c0fc107fafb722708477240be475edcc6397154f362\n",
            "Successfully built pyairports\n",
            "Installing collected packages: pyairports, uvloop, triton, sympy, pycountry, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, msgspec, interegular, httptools, diskcache, watchfiles, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, pydantic-extra-types, prometheus-fastapi-instrumentator, nvidia-cusolver-cu12, lm-format-enforcer, torch, ray, xformers, torchvision, outlines, mistral-common, compressed-tensors, vllm\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.5.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed compressed-tensors-0.8.0 diskcache-5.6.3 gguf-0.10.0 httptools-0.7.1 interegular-0.3.3 lm-format-enforcer-0.10.12 mistral-common-1.8.8 msgspec-0.20.0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opencv-python-headless-4.11.0.86 outlines-0.0.46 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 pyairports-0.0.1 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.53.0 sympy-1.13.1 torch-2.5.1 torchvision-0.20.1 triton-3.1.0 uvloop-0.22.1 vllm-0.6.4.post1 watchfiles-1.1.1 xformers-0.0.28.post3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "88b185b0674c44eb852ab3672ed750ea",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.46.1\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (0.7.0)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.1)\n",
            "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.1) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.1) (2025.11.12)\n",
            "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "Successfully installed tokenizers-0.20.3 transformers-4.46.1\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n"
          ]
        }
      ],
      "source": [
        "# --- RUN THIS IN GOOGLE COLAB (AFTER RESTARTING RUNTIME) ---\n",
        "\n",
        "# 1. Install specific versions that work with Qwen2-VL\n",
        "# We use vllm 0.6.4.post1 which contains the fix for the 'make_batched_images' error\n",
        "print(\"⏳ Installing dependencies... (This takes about 2-3 minutes)\")\n",
        "!pip install vllm==0.6.4.post1\n",
        "!pip install transformers==4.46.1\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "viQ0uJf1I8vQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "API URL IS: \n",
            "https://sternmost-nonesuriently-trinity.ngrok-free.dev/v1\n"
          ]
        }
      ],
      "source": [
        "# 2. Login to ngrok (Replace with your token)\n",
        "from pyngrok import ngrok\n",
        "# ENTER YOUR TOKEN BELOW\n",
        "ngrok.set_auth_token(\"36w3cwHtS4SvmUtDwo24VUcm22M_5AMVMsCmiibBMHYwfUNrc\")\n",
        "\n",
        "# 3. Open the Tunnel\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"\\nAPI URL IS: \\n{public_url}/v1\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5OGPeAhI-50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-27 12:20:55.991520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766838056.011955   15236 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766838056.018258   15236 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766838056.034644   15236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766838056.034670   15236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766838056.034674   15236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766838056.034679   15236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-27 12:20:56.039444: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 12-27 12:21:04 api_server.py:585] vLLM API server version 0.6.4.post1\n",
            "INFO 12-27 12:21:04 api_server.py:586] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2-VL-7B-Instruct-AWQ', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=8192, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='awq', rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\n",
            "INFO 12-27 12:21:04 api_server.py:175] Multiprocessing frontend to use ipc:///tmp/7ca75846-9ec2-4c2e-9b7c-fd367d4747ce for IPC Path.\n",
            "INFO 12-27 12:21:04 api_server.py:194] Started engine process with PID 15306\n",
            "2025-12-27 12:21:13.847702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766838073.885799   15306 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766838073.897132   15306 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766838073.924874   15306 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766838073.924919   15306 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766838073.924929   15306 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766838073.924937   15306 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 12-27 12:21:28 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
            "WARNING 12-27 12:21:28 config.py:428] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "WARNING 12-27 12:21:28 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
            "WARNING 12-27 12:21:28 config.py:503] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 12-27 12:21:41 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
            "WARNING 12-27 12:21:41 config.py:428] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "WARNING 12-27 12:21:41 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
            "WARNING 12-27 12:21:41 config.py:503] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 12-27 12:21:41 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='Qwen/Qwen2-VL-7B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2-VL-7B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2-VL-7B-Instruct-AWQ, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=True, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
            "INFO 12-27 12:21:42 selector.py:261] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 12-27 12:21:42 selector.py:144] Using XFormers backend.\n",
            "INFO 12-27 12:21:43 model_runner.py:1072] Starting to load model Qwen/Qwen2-VL-7B-Instruct-AWQ...\n",
            "INFO 12-27 12:21:43 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 2/2 [00:27<00:00, 13.60s/it]\n",
            "INFO 12-27 12:22:11 model_runner.py:1077] Loading model weights took 6.4639 GB\n",
            "INFO 12-27 12:22:21 worker.py:232] Memory profiling results: total_gpu_memory=14.74GiB initial_memory_usage=6.84GiB peak_torch_memory=7.84GiB memory_usage_post_profile=6.97GiB non_torch_memory=0.50GiB kv_cache_size=4.93GiB gpu_memory_utilization=0.90\n",
            "INFO 12-27 12:22:22 gpu_executor.py:113] # GPU blocks: 5768, # CPU blocks: 4681\n",
            "INFO 12-27 12:22:22 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 11.27x\n",
            "INFO 12-27 12:22:27 api_server.py:249] vLLM to use /tmp/tmpjs3r45if as PROMETHEUS_MULTIPROC_DIR\n",
            "INFO 12-27 12:22:27 launcher.py:19] Available routes are:\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /health, Methods: GET\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /tokenize, Methods: POST\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /detokenize, Methods: POST\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /v1/models, Methods: GET\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /version, Methods: GET\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /v1/completions, Methods: POST\n",
            "INFO 12-27 12:22:27 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m15236\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "INFO 12-27 12:22:37 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:22:47 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:22:57 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:23:07 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:23:17 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:23:27 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:23:37 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:23:47 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:23:57 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:24:07 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:24:17 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:24:27 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:24:37 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:24:41 logger.py:37] Received request chatcmpl-2e39958fb05847499bcd121be1dac265: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:24:41 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:24:43 engine.py:267] Added request chatcmpl-2e39958fb05847499bcd121be1dac265.\n",
            "INFO 12-27 12:24:49 metrics.py:449] Avg prompt throughput: 262.6 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:24:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45886 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:24:58 logger.py:37] Received request chatcmpl-65a0fdff135a47c082dc2a29a88cd8ad: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:24:58 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:24:58 engine.py:267] Added request chatcmpl-65a0fdff135a47c082dc2a29a88cd8ad.\n",
            "INFO 12-27 12:25:01 metrics.py:449] Avg prompt throughput: 235.5 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45886 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:25:02 logger.py:37] Received request chatcmpl-0996b59cebac4c4c9929b582eeb328bc: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:25:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:25:02 engine.py:267] Added request chatcmpl-0996b59cebac4c4c9929b582eeb328bc.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45886 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:25:05 logger.py:37] Received request chatcmpl-1e6170581ffb4b74a4776bea66b97694: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:25:05 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:25:05 engine.py:267] Added request chatcmpl-1e6170581ffb4b74a4776bea66b97694.\n",
            "INFO 12-27 12:25:07 metrics.py:449] Avg prompt throughput: 505.7 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45886 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:25:08 logger.py:37] Received request chatcmpl-e7dd7c4752204b918a0a91ddea872320: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:25:08 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:25:08 engine.py:267] Added request chatcmpl-e7dd7c4752204b918a0a91ddea872320.\n",
            "INFO 12-27 12:25:12 metrics.py:449] Avg prompt throughput: 371.3 tokens/s, Avg generation throughput: 10.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:25:17 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45886 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:25:19 logger.py:37] Received request chatcmpl-624ee2d2bfd54432baee8fa7f1435175: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:25:19 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:25:19 engine.py:267] Added request chatcmpl-624ee2d2bfd54432baee8fa7f1435175.\n",
            "INFO 12-27 12:25:25 metrics.py:449] Avg prompt throughput: 412.0 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:25:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45886 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:25:34 logger.py:37] Received request chatcmpl-82e71f50228643dea4fb1d7a59e514e1: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:25:34 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:25:35 engine.py:267] Added request chatcmpl-82e71f50228643dea4fb1d7a59e514e1.\n",
            "INFO 12-27 12:25:36 metrics.py:449] Avg prompt throughput: 179.3 tokens/s, Avg generation throughput: 16.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:25:41 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:25:46 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:25:51 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:25:56 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:01 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:06 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:11 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:16 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:21 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:26 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:31 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:36 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:41 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:46 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:51 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:26:56 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:01 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:06 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:11 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:16 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:22 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:27 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:32 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:37 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:42 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:47 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:52 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:27:57 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:28:02 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:28:07 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:28:12 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45886 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:28:24 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:28:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:28:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:28:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:29:04 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:29:09 logger.py:37] Received request chatcmpl-0ddc8fcac00b4f10aa070acc23a3314f: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:29:09 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:29:09 engine.py:267] Added request chatcmpl-0ddc8fcac00b4f10aa070acc23a3314f.\n",
            "INFO 12-27 12:29:15 metrics.py:449] Avg prompt throughput: 293.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:29:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:29:25 logger.py:37] Received request chatcmpl-e621177f12a1472a8b24f9b1ae3cf182: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:29:25 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:29:25 engine.py:267] Added request chatcmpl-e621177f12a1472a8b24f9b1ae3cf182.\n",
            "INFO 12-27 12:29:28 metrics.py:449] Avg prompt throughput: 220.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:29:28 logger.py:37] Received request chatcmpl-3346726f4f4844e0bb0daa30da4c89c0: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:29:28 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:29:28 engine.py:267] Added request chatcmpl-3346726f4f4844e0bb0daa30da4c89c0.\n",
            "INFO 12-27 12:29:33 metrics.py:449] Avg prompt throughput: 323.3 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:29:38 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:29:43 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:29:48 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:29:53 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:29:58 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:03 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:08 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:13 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:18 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:23 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:28 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:33 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:38 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:43 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:48 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:53 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:30:58 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:03 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:08 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:13 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:18 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:23 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:28 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:33 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:38 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:43 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:48 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:53 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:31:58 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:32:03 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:32:08 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:32:11 logger.py:37] Received request chatcmpl-fd49978ea2e148dfac81f58e3f23ee32: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:32:11 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:32:11 engine.py:267] Added request chatcmpl-fd49978ea2e148dfac81f58e3f23ee32.\n",
            "INFO 12-27 12:32:14 metrics.py:449] Avg prompt throughput: 299.3 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:32:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:32:23 logger.py:37] Received request chatcmpl-bb254ca1b9b44c94bebb2dcdc73362d8: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:32:23 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:32:23 engine.py:267] Added request chatcmpl-bb254ca1b9b44c94bebb2dcdc73362d8.\n",
            "INFO 12-27 12:32:29 metrics.py:449] Avg prompt throughput: 338.1 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:32:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:32:39 logger.py:37] Received request chatcmpl-30ae4dd0c23343ec8aea72659e4edd88: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:32:39 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:32:39 engine.py:267] Added request chatcmpl-30ae4dd0c23343ec8aea72659e4edd88.\n",
            "INFO 12-27 12:32:40 metrics.py:449] Avg prompt throughput: 170.0 tokens/s, Avg generation throughput: 16.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:32:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:32:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:32:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:33:00 logger.py:37] Received request chatcmpl-a83b35a7079642e1a36142da21f9a0f1: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:33:00 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:33:00 engine.py:267] Added request chatcmpl-a83b35a7079642e1a36142da21f9a0f1.\n",
            "INFO 12-27 12:33:01 metrics.py:449] Avg prompt throughput: 204.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:33:06 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:33:12 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:33:17 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:33:20 logger.py:37] Received request chatcmpl-0f547a5b99184958a2185c36d2bfa424: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:33:20 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:33:20 engine.py:267] Added request chatcmpl-0f547a5b99184958a2185c36d2bfa424.\n",
            "INFO 12-27 12:33:28 metrics.py:449] Avg prompt throughput: 284.9 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:33:33 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:33:37 logger.py:37] Received request chatcmpl-322a4e9988084a34b57d40db0dae0df9: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:33:37 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:33:37 engine.py:267] Added request chatcmpl-322a4e9988084a34b57d40db0dae0df9.\n",
            "INFO 12-27 12:33:40 metrics.py:449] Avg prompt throughput: 222.2 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:33:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:33:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:33:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:33:58 logger.py:37] Received request chatcmpl-0ddb874388cb4158a4d75f85f35d9213: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:33:58 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:33:58 engine.py:267] Added request chatcmpl-0ddb874388cb4158a4d75f85f35d9213.\n",
            "INFO 12-27 12:34:01 metrics.py:449] Avg prompt throughput: 300.1 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:34:06 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:34:11 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:34:16 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:34:18 logger.py:37] Received request chatcmpl-fde7615e6b3c45d0b26a458bf551d1ab: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:34:18 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:34:18 engine.py:267] Added request chatcmpl-fde7615e6b3c45d0b26a458bf551d1ab.\n",
            "INFO 12-27 12:34:25 metrics.py:449] Avg prompt throughput: 352.9 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:34:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:34:32 logger.py:37] Received request chatcmpl-3514a20d27ec4bf6bcfe9d7168494457: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:34:32 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:34:32 engine.py:267] Added request chatcmpl-3514a20d27ec4bf6bcfe9d7168494457.\n",
            "INFO 12-27 12:34:41 metrics.py:449] Avg prompt throughput: 313.5 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:34:46 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:34:51 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:34:56 logger.py:37] Received request chatcmpl-8005f201696548e99f0599d95cf5bde1: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:34:56 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:34:56 engine.py:267] Added request chatcmpl-8005f201696548e99f0599d95cf5bde1.\n",
            "INFO 12-27 12:35:02 metrics.py:449] Avg prompt throughput: 290.1 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:07 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:35:12 logger.py:37] Received request chatcmpl-ca035b40c4254cbebd1ac68d71c5a226: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:35:12 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:35:12 engine.py:267] Added request chatcmpl-ca035b40c4254cbebd1ac68d71c5a226.\n",
            "INFO 12-27 12:35:14 metrics.py:449] Avg prompt throughput: 234.3 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:24 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:35:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:35:56 logger.py:37] Received request chatcmpl-46b270ba84be4f959946e4c0c4c227c3: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:35:56 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:35:56 engine.py:267] Added request chatcmpl-46b270ba84be4f959946e4c0c4c227c3.\n",
            "INFO 12-27 12:35:59 metrics.py:449] Avg prompt throughput: 370.9 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:04 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:36:42 logger.py:37] Received request chatcmpl-6d065c9bac994ab6b7c065718bcd1e27: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:36:42 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:36:42 engine.py:267] Added request chatcmpl-6d065c9bac994ab6b7c065718bcd1e27.\n",
            "INFO 12-27 12:36:48 metrics.py:449] Avg prompt throughput: 370.3 tokens/s, Avg generation throughput: 3.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:53 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:36:58 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:37:00 logger.py:37] Received request chatcmpl-1fb088ad6fed4ece85f345e091456773: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:37:00 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:37:00 engine.py:267] Added request chatcmpl-1fb088ad6fed4ece85f345e091456773.\n",
            "INFO 12-27 12:37:03 metrics.py:449] Avg prompt throughput: 323.5 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:08 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:13 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:18 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:23 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:28 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:37:31 logger.py:37] Received request chatcmpl-f3f6ccdcd993409290018e35fa239427: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:37:31 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:37:31 engine.py:267] Added request chatcmpl-f3f6ccdcd993409290018e35fa239427.\n",
            "INFO 12-27 12:37:34 metrics.py:449] Avg prompt throughput: 347.7 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:37:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:38:04 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:38:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:38:10 logger.py:37] Received request chatcmpl-259f7a89bf2d4aaeb8cd88303e3b840c: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:38:10 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:38:10 engine.py:267] Added request chatcmpl-259f7a89bf2d4aaeb8cd88303e3b840c.\n",
            "INFO 12-27 12:38:17 metrics.py:449] Avg prompt throughput: 384.3 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:38:22 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:38:27 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:38:29 logger.py:37] Received request chatcmpl-0ce79568c7c94aba9aab1d2c02b7ae59: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:38:29 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:38:29 engine.py:267] Added request chatcmpl-0ce79568c7c94aba9aab1d2c02b7ae59.\n",
            "INFO 12-27 12:38:32 metrics.py:449] Avg prompt throughput: 322.7 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:38:37 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:38:42 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:38:45 logger.py:37] Received request chatcmpl-eaa179c55b90489cad5e976facb44527: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:38:45 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:38:45 engine.py:267] Added request chatcmpl-eaa179c55b90489cad5e976facb44527.\n",
            "INFO 12-27 12:38:48 metrics.py:449] Avg prompt throughput: 292.3 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:38:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:38:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:04 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:14 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:24 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:39:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:39:55 logger.py:37] Received request chatcmpl-3938c26956bf42ff8034635629a57e36: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:39:55 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:39:55 engine.py:267] Added request chatcmpl-3938c26956bf42ff8034635629a57e36.\n",
            "INFO 12-27 12:40:02 metrics.py:449] Avg prompt throughput: 392.2 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:40:07 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:40:12 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:40:13 logger.py:37] Received request chatcmpl-5b268bbceb3f44bf9870690429a470f9: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:40:13 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:40:13 engine.py:267] Added request chatcmpl-5b268bbceb3f44bf9870690429a470f9.\n",
            "INFO 12-27 12:40:17 metrics.py:449] Avg prompt throughput: 321.3 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:40:22 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:40:27 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:40:32 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:40:36 logger.py:37] Received request chatcmpl-529627c000d644798a135cea37e01f4a: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:40:36 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:40:36 engine.py:267] Added request chatcmpl-529627c000d644798a135cea37e01f4a.\n",
            "INFO 12-27 12:40:39 metrics.py:449] Avg prompt throughput: 248.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:40:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:40:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:40:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:41:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:41:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:41:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:41:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:41:18 logger.py:37] Received request chatcmpl-6b54c066757740658b7f14897f326965: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:41:18 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:41:18 engine.py:267] Added request chatcmpl-6b54c066757740658b7f14897f326965.\n",
            "INFO 12-27 12:41:25 metrics.py:449] Avg prompt throughput: 306.6 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:41:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:41:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:41:39 logger.py:37] Received request chatcmpl-993c5394bba048908a336322d1e8a5f3: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:41:39 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:41:39 engine.py:267] Added request chatcmpl-993c5394bba048908a336322d1e8a5f3.\n",
            "INFO 12-27 12:41:42 metrics.py:449] Avg prompt throughput: 241.2 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:41:47 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:41:51 logger.py:37] Received request chatcmpl-36ad2928bc23407da0354a3e71ba33f1: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:41:51 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:41:51 engine.py:267] Added request chatcmpl-36ad2928bc23407da0354a3e71ba33f1.\n",
            "INFO 12-27 12:41:54 metrics.py:449] Avg prompt throughput: 243.4 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:41:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:04 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:14 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:24 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:42:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:42:49 logger.py:37] Received request chatcmpl-841bb6e32d584e3b86554f7b2eff00e4: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:42:49 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:42:49 engine.py:267] Added request chatcmpl-841bb6e32d584e3b86554f7b2eff00e4.\n",
            "INFO 12-27 12:42:56 metrics.py:449] Avg prompt throughput: 284.1 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:43:01 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:43:04 logger.py:37] Received request chatcmpl-534ebaafb05641a8bfc35483aa23b98d: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:43:04 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:43:04 engine.py:267] Added request chatcmpl-534ebaafb05641a8bfc35483aa23b98d.\n",
            "INFO 12-27 12:43:07 metrics.py:449] Avg prompt throughput: 275.4 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:43:12 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:43:17 logger.py:37] Received request chatcmpl-237bce02cd9c4f30b04f1290873ca1c0: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:43:17 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:43:17 engine.py:267] Added request chatcmpl-237bce02cd9c4f30b04f1290873ca1c0.\n",
            "INFO 12-27 12:43:24 metrics.py:449] Avg prompt throughput: 268.6 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:43:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:43:35 logger.py:37] Received request chatcmpl-8f4ba581b0bc40c0808c22943e545e83: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:43:35 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:43:35 engine.py:267] Added request chatcmpl-8f4ba581b0bc40c0808c22943e545e83.\n",
            "INFO 12-27 12:43:37 metrics.py:449] Avg prompt throughput: 197.2 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:43:42 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:43:44 logger.py:37] Received request chatcmpl-6ec90b061937413f917d05a5f73b035b: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:43:44 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:43:44 engine.py:267] Added request chatcmpl-6ec90b061937413f917d05a5f73b035b.\n",
            "INFO 12-27 12:43:47 metrics.py:449] Avg prompt throughput: 369.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:43:52 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:43:58 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:44:01 logger.py:37] Received request chatcmpl-f674287c29c147da90c9e2e07a1d9635: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:44:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:44:01 engine.py:267] Added request chatcmpl-f674287c29c147da90c9e2e07a1d9635.\n",
            "INFO 12-27 12:44:07 metrics.py:449] Avg prompt throughput: 320.7 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:44:12 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:44:16 logger.py:37] Received request chatcmpl-cbe5971d051a4dd48d91d74095b32e5b: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:44:16 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:44:16 engine.py:267] Added request chatcmpl-cbe5971d051a4dd48d91d74095b32e5b.\n",
            "INFO 12-27 12:44:19 metrics.py:449] Avg prompt throughput: 244.2 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:44:23 logger.py:37] Received request chatcmpl-f57fab98039a4490b3c2826bc677fd3c: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:44:23 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:44:23 engine.py:267] Added request chatcmpl-f57fab98039a4490b3c2826bc677fd3c.\n",
            "INFO 12-27 12:44:26 metrics.py:449] Avg prompt throughput: 233.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:44:30 logger.py:37] Received request chatcmpl-d3bf9746a24b4b17bb0fb85c3770101a: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:44:30 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:44:30 engine.py:267] Added request chatcmpl-d3bf9746a24b4b17bb0fb85c3770101a.\n",
            "INFO 12-27 12:44:33 metrics.py:449] Avg prompt throughput: 246.9 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:44:37 logger.py:37] Received request chatcmpl-f6bb6cab01274b52a803e8a36aa433b1: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:44:37 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:44:37 engine.py:267] Added request chatcmpl-f6bb6cab01274b52a803e8a36aa433b1.\n",
            "INFO 12-27 12:44:40 metrics.py:449] Avg prompt throughput: 252.0 tokens/s, Avg generation throughput: 13.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:44:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:44:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:44:51 logger.py:37] Received request chatcmpl-47ce3f13c0d0452aa215c878a29101bf: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:44:51 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:44:51 engine.py:267] Added request chatcmpl-47ce3f13c0d0452aa215c878a29101bf.\n",
            "INFO 12-27 12:44:57 metrics.py:449] Avg prompt throughput: 428.3 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:02 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:45:07 logger.py:37] Received request chatcmpl-471e4d7010d744eea6ac622851070fe2: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:45:07 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:45:07 engine.py:267] Added request chatcmpl-471e4d7010d744eea6ac622851070fe2.\n",
            "INFO 12-27 12:45:10 metrics.py:449] Avg prompt throughput: 220.1 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:45:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:46:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:47:49 logger.py:37] Received request chatcmpl-4d78e2b8d7244b7d8df3d80d63f6429f: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:47:49 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:47:49 engine.py:267] Added request chatcmpl-4d78e2b8d7244b7d8df3d80d63f6429f.\n",
            "INFO 12-27 12:47:52 metrics.py:449] Avg prompt throughput: 280.3 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:47:57 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:48:02 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:48:07 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:48:12 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:48:17 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:48:22 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:48:27 logger.py:37] Received request chatcmpl-a75d6b46311e43de9d3f8c8e551a83d4: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:48:27 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:48:27 engine.py:267] Added request chatcmpl-a75d6b46311e43de9d3f8c8e551a83d4.\n",
            "INFO 12-27 12:48:34 metrics.py:449] Avg prompt throughput: 268.7 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:48:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:48:44 logger.py:37] Received request chatcmpl-a37a0864996f49369cd2cabc466be973: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:48:44 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:48:44 engine.py:267] Added request chatcmpl-a37a0864996f49369cd2cabc466be973.\n",
            "INFO 12-27 12:48:46 metrics.py:449] Avg prompt throughput: 213.9 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:48:51 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:48:56 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:49:01 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:49:06 logger.py:37] Received request chatcmpl-afedd5a00ac244ec8f1e96cdbdca1937: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:49:06 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:49:06 engine.py:267] Added request chatcmpl-afedd5a00ac244ec8f1e96cdbdca1937.\n",
            "INFO 12-27 12:49:09 metrics.py:449] Avg prompt throughput: 252.8 tokens/s, Avg generation throughput: 13.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:49:14 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:49:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:49:24 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:49:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:49:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:49:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:49:44 logger.py:37] Received request chatcmpl-48b0f0bbd23841c1abc6bccc3ea9e956: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:49:44 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:49:44 engine.py:267] Added request chatcmpl-48b0f0bbd23841c1abc6bccc3ea9e956.\n",
            "INFO 12-27 12:49:51 metrics.py:449] Avg prompt throughput: 253.5 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:49:57 logger.py:37] Received request chatcmpl-5c030a044bbb4b07807204aebfd3d6af: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:49:57 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:49:57 engine.py:267] Added request chatcmpl-5c030a044bbb4b07807204aebfd3d6af.\n",
            "INFO 12-27 12:50:04 metrics.py:449] Avg prompt throughput: 259.6 tokens/s, Avg generation throughput: 10.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:50:13 logger.py:37] Received request chatcmpl-30bfd1ba4a3c435dae4af22a6dcd6891: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:50:13 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:50:13 engine.py:267] Added request chatcmpl-30bfd1ba4a3c435dae4af22a6dcd6891.\n",
            "INFO 12-27 12:50:15 metrics.py:449] Avg prompt throughput: 175.7 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:50:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:51:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:52:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:52:53 logger.py:37] Received request chatcmpl-dc4c617ca7d340609a85fb3911cdc1bd: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Document Structure Layout Engine for financial and analytical documents.\\n\\n### CRITICAL RULES:\\n\\n1. **HEADER (Highest Priority)**\\n   - Section titles, table captions, chart titles\\n   - Bold/larger text that introduces a block\\n   - Example: \"Table 1: Economic Projections\" → HEADER\\n\\n2. **TABLE**\\n   - Rows and columns with data\\n   - Include column headers, exclude title if it\\'s distinct\\n\\n3. **FIGURE (HIGH PRIORITY - MUST DETECT)**\\n   - **Any visual element that is NOT text or tables**\\n   - Line charts, bar charts, box plots, scatter plots, pie charts\\n   - Graphs showing trends over time or comparisons\\n   - Diagrams, flowcharts, or illustrations\\n   - Shaded regions, colored areas, or plot elements\\n   - **IMPORTANT**: If you see axes (x/y), gridlines, data points, or trend lines → FIGURE\\n   - **IMPORTANT**: If there\\'s a visual representation of data → FIGURE\\n   - Look carefully for: chart legends, axis labels, plot markers, colored bars/lines\\n   - Figures often appear below headers like \"Figure 1:\", \"Chart:\", or numbered captions\\n   - **Do NOT miss figures - they contain critical information**\\n\\n4. **FOOTER**\\n   - Page numbers, footnotes, \"Confidential\" markers\\n\\n5. **TEXT**\\n   - Paragraphs, bullet points, explanatory text\\n\\n### MULTI-COLUMN DETECTION:\\nIf you detect multiple columns, add a \"column\" field (1, 2, etc.)\\n\\n### OUTPUT FORMAT (JSON only, no markdown):\\n{\\n  \"layout\": [\\n    {\"type\": \"HEADER\", \"bbox\": [50, 50, 100, 950], \"hint\": \"Table 1 Economic Projections\"},\\n    {\"type\": \"TABLE\", \"bbox\": [110, 50, 400, 950], \"column\": 1},\\n    {\"type\": \"HEADER\", \"bbox\": [410, 50, 460, 950], \"hint\": \"Figure 1: GDP Growth\"},\\n    {\"type\": \"FIGURE\", \"bbox\": [470, 50, 800, 950], \"hint\": \"Line chart showing GDP trends\"},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 50, 950, 450], \"column\": 1},\\n    {\"type\": \"TEXT\", \"bbox\": [810, 460, 950, 950], \"column\": 2}\\n  ]\\n}\\n\\nCoordinates: [ymin, xmin, ymax, xmax] scaled 0-1000, origin top-left.\\n\\n**REMINDER**: Always look for figures/charts - they are critical visual elements that must be detected!<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:52:53 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:52:53 engine.py:267] Added request chatcmpl-dc4c617ca7d340609a85fb3911cdc1bd.\n",
            "INFO 12-27 12:53:01 metrics.py:449] Avg prompt throughput: 303.4 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:06 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:53:11 logger.py:37] Received request chatcmpl-91dc1f2288d94a79a8b13f37218f6cde: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Table Data Extractor. Extract this table as STRICTLY VALID CSV.\\n\\n### REQUIREMENTS:\\n1. First row = column headers (no empty headers)\\n2. Numeric cells: Use the number or leave empty if unclear\\n3. Multi-level headers: Use underscore \"Parent_Child\"\\n4. Ranges like \"5.5-6.0\": Create TWO columns \"Variable_Min\" and \"Variable_Max\"\\n5. Merged cells: Repeat the value\\n6. NO markdown, NO explanations, NO code blocks\\n\\n### EXAMPLE:\\nInput: Table with \"GDP\" row showing \"2020: -6.5\" and range \"(-7.6 to -5.5)\"\\nOutput:\\nVariable,Year_2020_Median,Year_2020_Min,Year_2020_Max\\nGDP,-6.5,-7.6,-5.5\\n\\n### ERROR HANDLING:\\n- If cell is illegible: Leave empty\\n- If structure is unclear: Use best judgment but maintain CSV validity\\n\\nOutput ONLY the CSV text, starting immediately with the header row.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:53:11 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:53:11 engine.py:267] Added request chatcmpl-91dc1f2288d94a79a8b13f37218f6cde.\n",
            "INFO 12-27 12:53:13 metrics.py:449] Avg prompt throughput: 206.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:18 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:23 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:28 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:53:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:04 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:14 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:24 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:54 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:54:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:04 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:14 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:24 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:34 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:44 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:55:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:55:53 logger.py:37] Received request chatcmpl-1d90a6e262b14a1ebd3a20f7ebf18862: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nYou are a Chart Data Analyst. Extract structured data from this visualization.\\n\\n### OUTPUT FORMAT (JSON only):\\n{\\n  \"chart_type\": \"line|bar|box_plot|scatter|pie\",\\n  \"title\": \"Brief title\",\\n  \"axes\": {\\n    \"x_label\": \"Year\",\\n    \"y_label\": \"Percent\",\\n    \"x_values\": [\"2020\", \"2021\", \"2022\"],\\n    \"y_range\": [min, max]\\n  },\\n  \"data_series\": [\\n    {\\n      \"name\": \"Median GDP Growth\",\\n      \"points\": [\\n        {\"x\": \"2020\", \"y\": -6.5},\\n        {\"x\": \"2021\", \"y\": 5.0}\\n      ]\\n    }\\n  ],\\n  \"key_insights\": [\\n    \"GDP projected to decline 6.5% in 2020\",\\n    \"Recovery expected in 2021 with 5% growth\"\\n  ],\\n  \"data_labels\": [\"Any visible data labels on chart\"]\\n}\\n\\nCRITICAL: Extract ALL visible numbers from the chart. If exact values aren\\'t shown, estimate from axis.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
            "INFO 12-27 12:55:53 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n",
            "INFO 12-27 12:55:53 engine.py:267] Added request chatcmpl-1d90a6e262b14a1ebd3a20f7ebf18862.\n",
            "INFO 12-27 12:55:56 metrics.py:449] Avg prompt throughput: 270.8 tokens/s, Avg generation throughput: 11.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:56:01 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     ::1:45398 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 12-27 12:56:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:56:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:56:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:56:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:56:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:57:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:57:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:57:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:57:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:57:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:57:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:58:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:58:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:58:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:58:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:58:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:58:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:59:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:59:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:59:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:59:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:59:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 12:59:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:00:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:00:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:00:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:00:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:00:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:00:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:01:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:01:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:01:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:01:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:01:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:01:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:02:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:02:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:02:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:02:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:02:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:02:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:03:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:03:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:03:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:03:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:03:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:03:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:04:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:04:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:04:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:04:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:04:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:04:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:05:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:05:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:05:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:05:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:05:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:05:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:06:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:06:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:06:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:06:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:06:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:06:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:07:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:07:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:07:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:07:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:07:45 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:07:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:08:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:08:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:08:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:08:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:08:46 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:08:56 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:09:06 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:09:16 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:09:26 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:09:36 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:09:46 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-27 13:09:56 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
          ]
        }
      ],
      "source": [
        "!python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model Qwen/Qwen2-VL-7B-Instruct-AWQ \\\n",
        "  --quantization awq \\\n",
        "  --gpu-memory-utilization 0.90 \\\n",
        "  --max-model-len 8192 \\\n",
        "  --dtype half \\\n",
        "  --port 8000 \\\n",
        "  --trust-remote-code \\\n",
        "  --enforce-eager"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
